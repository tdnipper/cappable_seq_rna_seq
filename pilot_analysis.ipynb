{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing cappable-seq pilot experiment\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "Similar to the `dusp11_clip_seq` pipeline:\n",
    "1. Trim any adapter sequences using `cutadapt`\n",
    "2. Deplete any ribosomal RNA using `SortMeRNA`\n",
    "3. Align to hybrid WSN/human genome index using `STAR`\n",
    "4. Quantify transcripts using `Salmon`\n",
    "5. Measure differential expression using `DeSeq2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with mycoplasma contamination\n",
    "\n",
    "I processed this pipeline to the quantification step and had frustratingly low alignment percentages at both the genomic and transcriptomic levels. I BLAST'ed some of the unmapped reads and found mycoplasma contamination in all samples. Bad. To remove as many myco reads as possible, I'm including the myco_genome.fasta along with human_rRNAs.fasta during ribodepletion to pull out myco sequences as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make `filename-utils.py` module to allow repeated extraction of R1/R2 reads\n",
    "\n",
    "Sequencing core ran paired-end sequencing and returned foreward/reverse files for each sample (R1/R2). I'm going to make a script that will iterate over a folder and return each file in a dict with the sample name so samplename_R1 and samplename_R2 will be in a dict{samplename: [samplename_R1, samplename_R2]}. This can be called in subsequent files as a function to find all read files in a directory for a given program to run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load filename_utils.py - NOTE: this line imports the text of the file into this cell, \n",
    "# is not included in the actual file\n",
    "#! /usr/bin/env python3\n",
    "__name__ = \"filename_utils\"\n",
    "__author__ = \"Thomas\"\n",
    "__date__ = \"2024-05-08\" \n",
    "\n",
    "import os\n",
    "\n",
    "# Assumes prefixes are the same for read files with _R1 and _R2 denoting fwd and rev reads\n",
    "\n",
    "def get_filenames(directory):\n",
    "    replicates = {}\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for file in filenames:\n",
    "            if \".log\" not in file:\n",
    "                name = file.split(\"_R\")[0]\n",
    "                if name not in replicates:\n",
    "                    replicates[name] = []\n",
    "                replicates[name].append(file)\n",
    "\n",
    "    return replicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ribodeplete\n",
    "We shouldn't have many rRNA due to ribodepletion of inputs and the nature of cappable-seq samples, but will still use `sortmeRNA` to align to human rRNA sequences and see what comes out. (NOTE: also myco sequences in combined .fasta file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load run_sortmeRNA.py\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from filename_utils import get_filenames\n",
    "\n",
    "basedir=os.path.abspath(os.path.dirname(__file__))\n",
    "PUID = os.getuid()\n",
    "PGID = os.getgid()\n",
    "trimmed_reads_dir = f\"{basedir}/trimmed_reads\" #Changed for myco removal\n",
    "ribofile = f\"{basedir}/decon_reads.fasta\"\n",
    "sort_dir = f\"{basedir}/ribodepleted_reads\"\n",
    "\n",
    "if not os.path.exists(trimmed_reads_dir):\n",
    "    raise FileNotFoundError(f\"Error: trimmed_reads directory does not exist at {trimmed_reads_dir}\")\n",
    "\n",
    "if not os.path.exists(ribofile):\n",
    "    raise FileNotFoundError(f\"Error: human_rRNAs.fasta does not exist at {ribofile}\")\n",
    "\n",
    "if not os.path.exists(sort_dir):\n",
    "    os.mkdir(sort_dir)\n",
    "    os.chown(sort_dir, PUID, PGID)\n",
    "\n",
    "replicates = get_filenames(trimmed_reads_dir)\n",
    "\n",
    "for key in replicates:\n",
    "    subprocess.run([\"sortmerna\",\n",
    "                    \"-ref\",\n",
    "                    ribofile,\n",
    "                    \"-reads\",\n",
    "                    f\"{trimmed_reads_dir}/{replicates[key][0]}\",\n",
    "                    \"-reads\",\n",
    "                    f\"{trimmed_reads_dir}/{replicates[key][1]}\",\n",
    "                    \"-aligned\",\n",
    "                    f\"{sort_dir}/{key}_rRNA\", # label for RNA files\n",
    "                    \"-other\",\n",
    "                    f\"{sort_dir}/{key}_nonrRNA\", # label for nonrRNA files\n",
    "                    \"--paired_in\", # if either read aligns to rRNA, discard in aligned file\n",
    "                    \"--out2\", # write non-rRNA neads to 2 files\n",
    "                    \"-fastx\",\n",
    "                    \"--threads\",\n",
    "                    \"4\",\n",
    "                    \"-workdir\",\n",
    "                    f\"{sort_dir}/tmp\"])\n",
    "    \n",
    "    shutil.rmtree(f\"{sort_dir}/tmp/kvdb\")\n",
    "\n",
    "shutil.rmtree(f\"{sort_dir}/tmp\")\n",
    "\n",
    "# Rename files to match the naming convention used in the rest of the pipeline\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(sort_dir):\n",
    "    for filename in filenames:\n",
    "        if \"fwd\" in filename:\n",
    "            new_name = filename.replace(\"fwd\", \"R1\")\n",
    "            shutil.move(os.path.join(dirpath, filename), os.path.join(dirpath, new_name))\n",
    "        elif \"rev\" in filename:\n",
    "            new_name = filename.replace(\"rev\", \"R2\")\n",
    "            shutil.move(os.path.join(dirpath, filename), os.path.join(dirpath, new_name))\n",
    "        else:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment\n",
    "\n",
    "### Make `STAR` index\n",
    "\n",
    "Use concatenated WSN/FFLUC and hg38 genome to form a hybrid index for alignment. Use homemade WSN_annotated.gtf for flu and FFLUC spike in control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cat genome/GCF_000001405.40_GRCh38.p14_genomic.fasta genome/WSN_Mehle.fasta > hybrid_genome.fasta\n",
    "cat genome/genomic.gtf genome/WSN_annotated.gtf > genome/hybrid_annotated.gtf\n",
    "STAR --runThreadN 4 --runMode genomeGenerate --genomeDir genome/star_index --genomeFastaFiles genome/hybrid_genome.fasta --sjdbGTFfile genome/hybrid_annotated.gtf --sjdbOverhang 100 --limitGenomeGenerateRAM 30000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align to hybrid index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load run_star_pairedend.py\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from filename_utils import get_filenames\n",
    "\n",
    "# This script runs star on paired end R1 R2 reads after ribodepletion\n",
    "\n",
    "__name__ = \"run_star_pairedend\"\n",
    "__author__ = \"Thomas\"\n",
    "__date__ = \"2024-05-07\"\n",
    "\n",
    "basedir=os.path.abspath(os.path.dirname(__file__))\n",
    "PUID = os.getuid()\n",
    "PGID = os.getgid()\n",
    "reads_dir=f\"{basedir}/ribodepleted_reads\"\n",
    "\n",
    "if not os.path.exists(reads_dir):\n",
    "    raise FileNotFoundError(f\"Error: ribodepleted_reads directory does not exist at {reads_dir}\")\n",
    "\n",
    "star_dir = f\"{basedir}/star_alignment\"\n",
    "if not os.path.exists(star_dir):\n",
    "    os.mkdir(star_dir)\n",
    "    os.chown(star_dir, PUID, PGID)\n",
    "\n",
    "star_index_dir = f\"{basedir}/genome/star_index\"\n",
    "if not os.path.exists(star_index_dir):\n",
    "    raise FileNotFoundError(f\"Error: STAR index directory does not exist at {star_index_dir}\")\n",
    "\n",
    "reads = get_filenames(reads_dir)\n",
    "\n",
    "for key in reads:\n",
    "    if len(reads[key]) != 2:\n",
    "        raise ValueError(f\"Error: {key} does not have exactly 2 read files\")\n",
    "\n",
    "# Run star alignment on each sample in paired-end mode\n",
    "for key in reads:\n",
    "    if \"_nonrRNA\" in key:\n",
    "        keyname = key.replace(\"_nonrRNA\", \"\")\n",
    "        subprocess.run([\"STAR\",\n",
    "                        \"--runThreadN\",\n",
    "                        \"4\",\n",
    "                        \"--genomeDir\",\n",
    "                        star_index_dir,\n",
    "                        \"--readFilesIn\",\n",
    "                        f\"{reads_dir}/{reads[key][0]}\",\n",
    "                        f\"{reads_dir}/{reads[key][1]}\",\n",
    "                        \"--outFileNamePrefix\",\n",
    "                        f\"{star_dir}/{keyname}/{keyname}_\",\n",
    "                        \"--quantMode\",\n",
    "                        \"TranscriptomeSAM\",\n",
    "                        \"--genomeLoad\",\n",
    "                        \"LoadAndKeep\",\n",
    "                        \"--outReadsUnmapped\",\n",
    "                        \"Fastx\",\n",
    "                        \"--readFilesCommand\",\n",
    "                        \"zcat\"])\n",
    "        # Sort and index bam files\n",
    "        subprocess.run([\"samtools\",\n",
    "                        \"view\",\n",
    "                        f\"{star_dir}/{keyname}/{keyname}_Aligned.out.sam\",\n",
    "                        \"-o\",\n",
    "                        f\"{star_dir}/{keyname}/{keyname}_aligned.bam\"])\n",
    "        os.remove(f\"{star_dir}/{keyname}/{keyname}_Aligned.out.sam\")\n",
    "        subprocess.run([\"samtools\",\n",
    "                        \"sort\",\n",
    "                        f\"{star_dir}/{keyname}/{keyname}_Aligned.toTranscriptome.out.bam\",\n",
    "                        \"-o\",\n",
    "                        f\"{star_dir}/{keyname}/{keyname}_Aligned.toTranscriptome.sorted.bam\",\n",
    "                        \"-@\",\n",
    "                        \"4\"])\n",
    "        subprocess.run([\"samtools\",\n",
    "                        \"sort\",\n",
    "                        f\"{star_dir}/{keyname}/{keyname}_aligned.bam\",\n",
    "                        \"-o\",\n",
    "                        f\"{star_dir}/{keyname}/{keyname}_coord_sorted.bam\",\n",
    "                        \"-@\",# for dirpath, dirnames, filenames in os.walk(reads_dir):\n",
    "                        \"4\"])\n",
    "        subprocess.run([\"samtools\",\n",
    "                        \"index\",\n",
    "                        f\"{star_dir}/{keyname}/{keyname}_Aligned.toTranscriptome.sorted.bam\",\n",
    "                        \"-@\",\n",
    "                        \"4\"])\n",
    "        subprocess.run([\"samtools\",\n",
    "                        \"index\",\n",
    "                        f\"{star_dir}/{keyname}/{keyname}_coord_sorted.bam\",\n",
    "                        \"-@\",\n",
    "                        \"4\"])\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(f\"{star_dir}/exit\"):\n",
    "    subprocess.run([\"STAR\",\n",
    "                    \"--genomeLoad\", \n",
    "                    \"Remove\",\n",
    "                    \"--genomeDir\",\n",
    "                    star_index_dir,\n",
    "                    \"--outFileNamePrefix\",\n",
    "                    f\"{star_dir}/exit/exit\"])\n",
    "    shutil.rmtree(f\"{star_dir}/exit\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Quantify Reads directly from STAR output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify Reads- Alignment independent (bypass `star`)\n",
    "\n",
    "Now we need to quantify reads before counting them. `DeSeq2` can take counts from `salmon` (in fact, this is the official recommendation in the docs). In `dusp_11_clip-seq` I discussed quantification methods including simple gene counts and more sophisticated transcript counts. We'll use the genome alignments to inform transcript quantification (and maybe test alignment-independent quantification once this approach is done) in preparation to pass those results to `deseq2`.\n",
    "\n",
    "`Salmon` needs files aligned to the transcriptome, luckily `star` can output this. It first aligns to genome and them maps these alignments to the transcriptome, but still preserves any novel reads that don't map the provided transcriptome. (Note, this is different than the clip-seq approach because we simply wanted to find peaks in those files, not quantify reads.) However, this is an alignment-dependent approach.\n",
    "\n",
    "`Salmon` can quantify in an alignment-independent manner, and this method seems to be more accurate than alignment-dependent methods, counterintuitively. This is because using alignment-dependent methods (cufflinks, HTseq, FeatureCounts, maybe salmon in alignment-dependent mode) can vastly underestimate abundance from reads with >90% sequence similarity.\n",
    "\n",
    "Kind of confusingly, while alignment-independent mapping is superior for quantification, these counts are more accurate when mapped to the genome instead of the transcriptome. This is because transcripts can have conserved UTRs and sequences as well as different spliceoforms. So the best approach here I think is to quantify in an alignment-independent manner and collapse these counts down to the gene level. This is recommended here.\n",
    "\n",
    "To accomplish this, I'll use salmon to map reads to transcripts without aligning, and then collapse these into counts per gene before passing to `deseq2`. This makes `star` alignment obsolete. We need a transcript index for our hybrid genome for human/WSN transcripts. This is tricky. `gffread` can take a gff file and a genome file and extract transcripts from the genome sequences using the info in the gff file, outputting a transcripts.fasta file. Simply combining the gff files and the genome files results in issues with `gffread` only extracting either human or WSN sequences for some reason. To get around this I ran `gffread` on the WSN genome using the WSN gff and the human genome using the human gff and then combined those. This should work with the combined human/WSN genome that I made for `star` to make indexes for `salmon`.\n",
    "\n",
    "### Make `salmon` index\n",
    "\n",
    "Now we need to index the transcriptome (WSN and human) so that salmon can map to it. First use `gffread` to use the gff files I downloaded/created for both genomes to extract only the transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gffread -w genome/WSN_transcripts.fasta -g genome/WSN_Mehle.fasta genome/WSN_annotated.gtf\n",
    "gffread -w genome/human_transcripts.fasta -g genome/GCF_000001405.40_GRCh38.p14_genomic.fasta genome/genomic.gtf\n",
    "cat genome/WSN_transcripts.fasta genome/human_transcripts.fasta > genome/hybrid_transcripts.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: concatenating the gff files and genome files and then running gffread results in either WSN transcripts only or human transcripts only. Probably a formatting issue with homemade gff file. Instead, extract transcripts individually and concatenate them together.\n",
    "\n",
    "Also, WSN_transcripts.fasta contains HA and NA gene segments for some reason. Manually remove them so they don't break downstream processing and figure out why gffread is including them later.\n",
    "\n",
    "Now we can make the `salmon` index using the transcripts and masking the genome for more accurate mapping and automatic filtering of DNA contamination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# First make a list of genomic decoys\n",
    "grep \"^>\" >(cat genome/hybrid_genome.fasta) | cut -d \" \" -f 1 > genome/decoys.txt\n",
    "sed -i.bak -e 's/>//g' genome/decoys.txt\n",
    "\n",
    "# Make a gentrome file of transcriptome FIRST followed by genome to form the index\n",
    "cat genome/hybrid_transcripts.fasta genome/hybrid_genome.fasta > genome/hybrid_gentrome.fasta\n",
    "\n",
    "# Make salmon index\n",
    "salmon index -t genome/hybrid_gentrome.fasta -d genome/decoys.txt -p 4 -i genome/salmon_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run `Salmon`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load find_salmon_replicates.py\n",
    "#! /usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from filename_utils import get_filenames\n",
    "\n",
    "__name__ = \"find_salmon_replicates\"\n",
    "__author__ = \"Thomas\"\n",
    "__date__ = \"2024-05-07\"\n",
    "\n",
    "# Use this file to find all replicates denoted by _R(rep number)_ for each sample\n",
    "# and run Salmon quantification on them. This script will output a directory\n",
    "# containing the Salmon output files for each sample with replicates combined.\n",
    "\n",
    "# Define the path to the directory containing the Salmon output files\n",
    "basedir=os.path.abspath(os.path.dirname(__file__))\n",
    "PUID = os.getuid()\n",
    "PGID = os.getgid()\n",
    "salmon_dir = f\"{basedir}/salmon_quantification\"\n",
    "reads_dir = f\"{basedir}/ribodepleted_reads/\"\n",
    "salmon_index_dir = f\"{basedir}/genome/salmon_index/\"\n",
    "\n",
    "if not os.path.exists(salmon_dir):\n",
    "    os.mkdir(salmon_dir)\n",
    "    os.chown(salmon_dir, PUID, PGID)\n",
    "\n",
    "if not os.path.exists(reads_dir):\n",
    "    sys.exit(\"Error: ribodepleted_reads directory does not exist\")\n",
    "\n",
    "if not os.path.exists(salmon_index_dir):\n",
    "    print(\"Error: Salmon index directory does not exist\")\n",
    "    print(\"attempting salmon index\")\n",
    "    subprocess.run([\"salmon\", \n",
    "                    \"index\", \n",
    "                    \"-t\",\n",
    "                    \"genome/hybrid_gentrome.fasta\",\n",
    "                    \"-d\", \n",
    "                    \"genome/decoys.txt\",\n",
    "                    \"-p\",\n",
    "                    \"4\", \n",
    "                    \"-i\", \n",
    "                    salmon_index_dir])\n",
    "\n",
    "\n",
    "# Find replicate files in reads_dir and run salmon quantification on them, \n",
    "# combining the replicates for each sample\n",
    "\n",
    "replicates = get_filenames(reads_dir)\n",
    "\n",
    "for key in replicates:\n",
    "    if \"_nonrRNA\" in key:\n",
    "        subprocess.run([\"salmon\",\n",
    "                        \"quant\",\n",
    "                        \"-i\",\n",
    "                        salmon_index_dir,\n",
    "                        \"-l\",\n",
    "                        \"ISF\", # This might not be correct or consistent, check log files\n",
    "                        # f\"<(cat {reads_dir + replicates[key][0]} {reads_dir + replicates[key][1]})\"\n",
    "                        \"-1\",\n",
    "                        reads_dir + replicates[key][0],\n",
    "                        \"-2\",\n",
    "                        reads_dir + replicates[key][1],\n",
    "                        \"--validateMappings\",\n",
    "                        \"-p\",\n",
    "                        \"4\",\n",
    "                        \"--seqBias\",\n",
    "                        \"--gcBias\",\n",
    "                        \"--reduceGCMemory\",\n",
    "                        \"--writeUnmappedNames\",\n",
    "                        \"-o\",\n",
    "                        salmon_dir + \"/\" + key.strip(\"_nonrRNA\")])\n",
    "        \n",
    "# Rename the output files to include the sample name\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(salmon_dir):\n",
    "    for file in filenames:\n",
    "        if \"quant.sf\" in file:\n",
    "            sample_name = os.path.basename(dirpath)\n",
    "            os.rename((os.path.join(dirpath, file)),(os.path.join(dirpath, sample_name + \"_quant.sf\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salmon returns really low mapping percentages from running on an hg38 index (NOTE: this is still true after myco/ribodepletion, but less so). Maybe this is because many of our hits are mapping to unannotated transcripts (e.g Pol III stuff)? To deal with this, lets assemble a transcriptome *de novo* using reads that we obtained from star. This is called genome-guided assembly and can be done using `Trinity`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `Trinity` to generate a transcriptome *de novo*\n",
    "\n",
    "[Trinity](https://www.nature.com/articles/nbt.1883) can assmeble a transcriptome using RNA-seq reads and nothing else. This is really helpful if you don't have a reference genome. It can also assemble a transcriptome *after* aligning to the genome using something like `star`. This is useful if you do have a reference genome but are looking for unannotated transcripts as genome guided assembly may be [more accurate](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1911-6).\n",
    "\n",
    "To do this, first concatenate all .bam files from star_alignment into one mega bam file for trinity. **Triniy *requires* that you create a single assembly from all samples and then quantify each sample individually againsta it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load merge_star_bam.py\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "PUID = os.getuid()\n",
    "PGID = os.getgid()\n",
    "\n",
    "if not os.path.exists(\"trinity\"):\n",
    "    os.makedirs(\"trinity\")\n",
    "    os.chown(\"trinity\", PUID, PGID)\n",
    "\n",
    "if not os.path.exists(\"star_alignment\"):\n",
    "    raise FileNotFoundError(\"Star alignment directory not at './star_alignment'\")\n",
    "\n",
    "# Get a list of bam files from star_alignment\n",
    "bam_files = []\n",
    "for dirpath, dirnames, filenames in os.walk(\"star_alignment\"):\n",
    "    for filename in filenames:\n",
    "        if \"_aligned.bam\" in filename:\n",
    "            bam_files.append(os.path.join(dirpath, filename))\n",
    "\n",
    "# Merge bam files\n",
    "subprocess.run([\"samtools\",\n",
    "                \"merge\",\n",
    "                \"-o\",\n",
    "                \"trinity/merged.bam\",\n",
    "                *bam_files\n",
    "                ])\n",
    "\n",
    "# Sort bam file by coordinate\n",
    "subprocess.run([\"samtools\",\n",
    "                \"sort\",\n",
    "                \"-o\",\n",
    "                \"trinity/merged_sorted.bam\",\n",
    "                \"trinity/merged.bam\",\n",
    "                \"-@\",\n",
    "                \"4\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the run trinity on the large sorted .bam file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# %load run_trinity.sh\n",
    "#! /bin/bash\n",
    "\n",
    "# genome guided trinity on merged bam file\n",
    "\n",
    "Trinity --genome_guided_bam trinity/merged_sorted.bam \\\n",
    "    --genome_guided_max_intron 1000000 \\\n",
    "    --max_memory 30G \\\n",
    "    --CPU 4 \\\n",
    "    --output trinity/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes 36 hours and generates a directory with a `Trinity-GG.fasta` file containing assembled transcripts. Let's run salmon quantification of both raw reads and aligned reads against this new transcriptome as an index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantify using salmon against the trinity generated transcriptome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load salmon_trinity.py\n",
    "#! /usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from filename_utils import get_filenames\n",
    "\n",
    "__name__ = \"salmon_trinity\"\n",
    "__author__ = \"Thomas\"\n",
    "__date__ = \"2024-05-013\"\n",
    "\n",
    "salmon_index_dir = \"trinity/salmon_index\"\n",
    "basedir = os.path.abspath(os.path.dirname(__file__))\n",
    "PUID = os.getuid()\n",
    "PGID = os.getgid()\n",
    "salmon_dir = f\"{basedir}/salmon_quantification_trinity\"\n",
    "reads_dir = f\"{basedir}/ribodepleted_reads\"\n",
    "trinity_file = f\"{basedir}/trinity/Trinity-GG.fasta\"\n",
    "genome_dir = f\"{basedir}/genome\"\n",
    "\n",
    "# Use this script to run Salmon quantification on the Trinity assembled transcripts\n",
    "\n",
    "# Check for trinity directory\n",
    "if not os.path.exists(\"trinity\"):\n",
    "    raise FileNotFoundError(\"Error: trinity directory does not exist at ./trinity!\")\n",
    "else:\n",
    "    os.chown(\"trinity\", PUID, PGID)\n",
    "\n",
    "# Check for Trinity-GG.fasta\n",
    "if not os.path.exists(\"trinity/Trinity-GG.fasta\"):\n",
    "    raise FileNotFoundError(\"Error: Trinity-GG.fasta not found in ./trinity!\")\n",
    "\n",
    "# Check if trinity specific salmon index directory exists\n",
    "# If not, create it using trinity_decoys.txt\n",
    "# If trinity_decoys.txt does not exist, create it\n",
    "if not os.path.exists(salmon_index_dir):\n",
    "    print(\"trinity/salmon_index directory does not exist!\")\n",
    "    print(\"Attempting to create trinity/salmon_index directory...\")\n",
    "    os.mkdir(salmon_index_dir)\n",
    "    os.chown(salmon_index_dir, PUID, PGID)\n",
    "   \n",
    "    # Check for hybrid decoys.txt from hg38 and WSN genomes\n",
    "    if not os.path.exists(f\"{genome_dir}/decoys.txt\"):\n",
    "        print(\"decoys.txt not found, creating...\")\n",
    "        result = subprocess.run(\n",
    "            [\n",
    "                \"grep\",\n",
    "                \"'^>'\",\n",
    "                f\"<(cat {genome_dir}/hybrid_genome.fasta) | cut -d ' ' -f 1 > {genome_dir}/decoys.txt\",\n",
    "            ]\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            raise RuntimeError(\"Error: Failed to create trinity_decoys.txt!\")\n",
    "        result = subprocess.run(\n",
    "            [\"sed\", \"-i.bak\", \"-e\", \"s/>//g\", f\"{genome_dir}/decoys.txt\"]\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            raise RuntimeError(\"Error: Failed to edit trinity_decoys.txt!\")\n",
    "    \n",
    "    if not os.path.exists(trinity_file):\n",
    "        raise FileNotFoundError(\"Error: Trinity-GG.fasta not found!\")\n",
    "    \n",
    "    # Create hybrid gentrome out of trinity and hybrid genome from hg38 and WSN\n",
    "    if not os.path.exists(f\"{genome_dir}/hybrid_genome.fasta\"):\n",
    "        raise FileNotFoundError(\"Error: hybrid_genome.fasta not found!\")\n",
    "    \n",
    "    if not os.path.exists(\"trinity/trinity_gentrome.fasta\"):\n",
    "        print(\"trinity_gentrome.fasta not found, creating...\")\n",
    "        result = subprocess.run(\n",
    "            f\"cat {trinity_file} {genome_dir}/hybrid_genome.fasta > trinity/trinity_gentrome.fasta\",\n",
    "            shell=True,\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            raise RuntimeError(\"Error: Failed to create trinity_gentrome.fasta!\")\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        [\n",
    "            \"salmon\",\n",
    "            \"index\",\n",
    "            \"-t\",\n",
    "            \"trinity/trinity_gentrome.fasta\",\n",
    "            \"-d\",\n",
    "            f\"{genome_dir}/decoys.txt\",\n",
    "            \"-i\",\n",
    "            salmon_index_dir,\n",
    "            \"-p\",\n",
    "            \"4\",\n",
    "        ]\n",
    "    )\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(\"Error: Failed to create trinity salmon index!\")\n",
    "\n",
    "if not os.path.exists(salmon_dir):\n",
    "    os.mkdir(salmon_dir)\n",
    "    os.chown(salmon_dir, PUID, PGID)\n",
    "\n",
    "# Find all the fastq files in the ribodepleted directory\n",
    "files = get_filenames(reads_dir)\n",
    "\n",
    "# Run salmon quantification aligning to Trinity assembled transcripts\n",
    "for key in files:\n",
    "    if \"_nonrRNA\" in key:\n",
    "        result = subprocess.run(\n",
    "            [\n",
    "                \"salmon\",\n",
    "                \"quant\",\n",
    "                \"-i\",\n",
    "                salmon_index_dir,\n",
    "                \"-l\",\n",
    "                \"ISF\",\n",
    "                \"-1\",\n",
    "                f\"{reads_dir}/{files[key][0]}\",\n",
    "                \"-2\",\n",
    "                f\"{reads_dir}/{files[key][1]}\",\n",
    "                \"-p\",\n",
    "                \"4\",\n",
    "                \"-o\",\n",
    "                f\"{salmon_dir}/{key}\",\n",
    "            ]\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            raise RuntimeError(f\"Error: Failed to run salmon quantification on {key}!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipseq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
